{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a4b4ca-d31f-45cc-a7b2-ee259ca7b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, IntervalStrategy\n",
    "\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c4a26f-3fcc-4245-93bc-8fb4f3407588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "#modelName = \"./results/checkpoint-22428\"\n",
    "# modelName = \"EleutherAI/gpt-neo-125M\"\n",
    "#modelName = \"EleutherAI/gpt-neo-1.3B\"\n",
    "modelName = \"EleutherAI/gpt-neo-2.7B\"\n",
    "#modelName = \"EleutherAI/gpt-j-6B\"# , revision=\"sharded\")\n",
    "#modelName = \"EleutherAI/gpt-neox-20b\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName, bos_token='<|startoftext|>',\n",
    "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = AutoModelForCausalLM.from_pretrained(modelName).cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5b35c-f3f2-4471-93f2-2cbd96f07a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "runName = modelName + 'dialogSum'\n",
    "wandb.init(name=runName, project='gptNeo_dialogSum_ScaleStudy', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e48660-44e9-425f-b13b-2a4bb31a1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFiles = pd.read_json('../11_dialogsum/dialogsum/DialogSum_Data/dialogsum.train.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06182c2e-5a41-4f66-bdc5-6411c54a8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "texts = trainFiles['dialogue'] + \"\\nSUMMARY: \\n\" + trainFiles['summary']\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "114c7247-f637-408a-978c-b5c9d19574ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 1024\n",
    "\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ad2fdd-cb69-4316-a83a-d07d1f0ff3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    #print(pred)\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    orig_label_str = label_str\n",
    "    try:\n",
    "        pred_str = [oneStr[oneStr.find('SUMMARY:')+9:] for oneStr in pred_str]\n",
    "        label_str = [oneStr[oneStr.find('SUMMARY:')+9:] for oneStr in label_str]\n",
    "    except:\n",
    "        pred_str = pred_str\n",
    "        label_str = label_str\n",
    "        \n",
    "        \n",
    "    \n",
    "    rouge_output = metric.compute(\n",
    "        predictions=pred_str, references=label_str, rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        #\"rouge1_precision\": round(rouge_output['rouge1'].mid.precision, 4),\n",
    "        #\"rouge1_recall\": round(rouge_output['rouge1'].mid.recall, 4),\n",
    "        \"rouge1_fmeasure\": round(rouge_output['rouge1'].mid.fmeasure, 4),\n",
    "        #\"rouge2_precision\": round(rouge_output['rouge2'].mid.precision, 4),\n",
    "        #\"rouge2_recall\": round(rouge_output['rouge2'].mid.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output['rouge2'].mid.fmeasure, 4),\n",
    "        #\"rougeL_precision\": round(rouge_output['rougeL'].mid.precision, 4),\n",
    "        #\"rougeL_recall\": round(rouge_output['rougeL'].mid.recall, 4),\n",
    "        \"rougeL_fmeasure\": round(rouge_output['rougeL'].mid.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ed0f67-6504-4a27-b7d2-a8f375120d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def preprocess_logits_for_metrics(logits, labels):\n",
    "            if isinstance(logits, tuple):\n",
    "                # Depending on the model and config, logits may contain extra tensors,\n",
    "                # like past_key_values, but logits always come first\n",
    "                logits = logits[0]\n",
    "            return logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c27380-438b-47e2-9453-1235768894b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = DialogDataset(texts, tokenizer, max_length=max_length)\n",
    "\n",
    "\n",
    "\n",
    "evalLength = 10\n",
    "testFiles = pd.read_json('../11_dialogsum/dialogsum/DialogSum_Data/dialogsum.test.jsonl', lines=True)\n",
    "dialogueOnly = testFiles[:evalLength]['dialogue'] + \"\\nSUMMARY: \\n\" \n",
    "realSummaries = testFiles[:evalLength]['summary1']\n",
    "\n",
    "\n",
    "testDataset = DialogDataset(dialogueOnly, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3965362-aaa9-4c6c-bb81-185263588700",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_size = int(0.99 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "training_args = TrainingArguments(output_dir='./results',\n",
    "                                  num_train_epochs=5, \n",
    "                                  logging_steps=5000,\n",
    "                                  save_strategy=\"epoch\", \n",
    "                                  save_total_limit=5,\n",
    "                                  per_device_train_batch_size=batch_size, \n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  warmup_steps=100, \n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  fp16=True, #formerly true\n",
    "                                  #eval_steps=1000,\n",
    "                                  deepspeed='./dsconfig.json', #uncomment for 2.7B and other deepspeed runs\n",
    "                                  logging_dir='./logs')\n",
    "\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "                  args=training_args, \n",
    "                  train_dataset=train_dataset,        \n",
    "                  eval_dataset=val_dataset, \n",
    "                  preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b27d7-94d7-4ad3-afeb-c470e760815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log='all')\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2da3b-d0b0-420d-bd8a-10fbcf96f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0e563-f48e-47dd-ac49-0aa124fcd99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94b883-5796-4cf0-b39b-260868b735de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table_rows = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logs = {}\n",
    "\n",
    "\n",
    "sampleSumm = dict()\n",
    "#r1 = np.zeros((len(dialogueOnly)))\n",
    "#r2 = np.zeros((len(dialogueOnly)))\n",
    "#rL = np.zeros((len(dialogueOnly)))\n",
    "#rLsum = np.zeros((len(dialogueOnly)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file in range(len(dialogueOnly)):\n",
    "    text = dialogueOnly[file]\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "    summary_candidate = model.generate(tokenized_text, do_sample=True, top_k=50,\n",
    "                                bos_token='<|startoftext|>',\n",
    "                                eos_token='<|endoftext|>', pad_token='<|pad|>',\n",
    "                                max_length=1024, top_p=0.95, temperature=1.9)\n",
    "    summary_candidate_decoded = tokenizer.decode(summary_candidate[0], skip_special_tokens=True)\n",
    "\n",
    "    sampleSumm['prompt'] = text\n",
    "    sampleSumm['summary candidate'] = summary_candidate_decoded[int(len(text)):]\n",
    "    sampleSumm['real summary'] = realSummaries[file]\n",
    "    print(sampleSumm)\n",
    "\n",
    "    table_rows.append([list(r) for r in zip([sampleSumm['prompt']], [sampleSumm['summary candidate']],[sampleSumm['real summary']])][0])\n",
    "    #print(table_rows)\n",
    "    tablename = 'sampleSummaries'\n",
    "    logs.update({tablename:wandb.Table(\n",
    "        columns=['query', 'response', 'actual summary'],\n",
    "        rows=table_rows)})\n",
    "\n",
    "\n",
    "    #metrics = metric.compute(predictions=[summary_candidate_decoded[int(len(text)):]], references=[realSummaries.loc[file]])\n",
    "\n",
    "    #r1[file] = metrics['rouge1'][0][2]\n",
    "    #r2[file] = metrics['rouge2'][0][2]\n",
    "    #rL[file] = metrics['rougeL'][0][2]\n",
    "    #rLsum[file] = metrics['rougeLsum'][0][2]\n",
    "    #wandb.log({'epoch': epoch,'eval/ROUGE1':np.mean(r1),\n",
    "    #'eval/ROUGE2' = np.mean(r2),\n",
    "    #'eval/ROUGEL' = np.mean(rL),\n",
    "    #'eval/ROUGELsum' = np.mean(rLsum)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99fc8a8-91cb-49af-94fa-ac546b1f984d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f08124-3754-488d-8ac7-e449cf0cefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer(\"<|startoftext|>\", return_tensors=\"pt\").input_ids.cuda()\n",
    "sample_outputs = model.generate(generated, do_sample=True, top_k=50,\n",
    "                                bos_token='<|startoftext|>',\n",
    "                                eos_token='<|endoftext|>', pad_token='<|pad|>',\n",
    "                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=20)\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ec139-b66e-4761-890a-7821fbcc6c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d29de3-ee35-4fa6-ad71-f39231ba6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logs['sampleSummaries']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ea9e4-2dbb-484f-ae36-4816e94b16aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
